{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wrappers_ben as wb\n",
    "from lib import dqn_model_ben as dmb\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "\n",
    "#Hyparameter\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "# EPSILON_START = 1.0\n",
    "EPSILON_START = 0.02\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_FINAL = 0.02\n",
    "EPISODES = 1000000\n",
    "START_STEPS = 3000\n",
    "COPY_STEP = 1000\n",
    "REPORT_EVERY_STEP = 50000\n",
    "REPLAY_START_SIZE = 3000\n",
    "\n",
    "Experience = namedtuple('Experience', field_names=['obs', 'action', 'reward', 'done', 'next_obs'])\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.exp_buffer = collections.deque(maxlen=REPLAY_START_SIZE)\n",
    "        self._reset()\n",
    "    def _reset(self):\n",
    "        self.current_obs = self.env.reset()\n",
    "        self.total_reward = 0.0\n",
    "    def get_Qas(self, model, obs):\n",
    "        Qas = model.predict(np.array([obs]), 1)\n",
    "        return Qas\n",
    "    def select_epision_greedy_action(self, model, obs, epsilon):\n",
    "        Qas = self.get_Qas(model, obs)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(np.squeeze(Qas))\n",
    "        return action\n",
    "    def play_step(self, model, epsilon):\n",
    "        action = self.select_epision_greedy_action(model, self.current_obs, epsilon)\n",
    "        new_obs,reward,done,_ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        exp = Experience(self.current_obs, action, reward, done, new_obs)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.current_obs = new_obs\n",
    "        if done:\n",
    "            print(\"self.total_reward:\", self.total_reward)\n",
    "            self._reset()\n",
    "    def test(self, DEFAULT_ENV_NAME, model, i_epoch):\n",
    "        env = wb.make_env(DEFAULT_ENV_NAME)\n",
    "        env = gym.wrappers.Monitor(env,\"recording\"+str(i_epoch), force=True)\n",
    "        current_obs = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = self.select_epision_greedy_action(model, current_obs, 0)\n",
    "            new_obs,reward,done,_ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            current_obs = new_obs\n",
    "        env.env.close()\n",
    "        return total_reward\n",
    "    def get_exp_buffer(self):\n",
    "        test = np.array(self.exp_buffer)\n",
    "        return self.exp_buffer\n",
    "        \n",
    "        \n",
    "def sample_memories2(buffer, batch_size):\n",
    "        indices = np.random.choice(len(buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "    \n",
    "def one_hot(target, shape):\n",
    "    b = np.zeros(shape)\n",
    "    b[np.arange(shape[0]), target] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 84, 84, 4)]       0         \n",
      "_________________________________________________________________\n",
      "layer1 (Conv2D)              (None, 21, 21, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "layer2 (Conv2D)              (None, 11, 11, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "layer3 (Conv2D)              (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 512)               3965440   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,046,502\n",
      "Trainable params: 4,046,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "self.total_reward: -20.0\n",
      "self.total_reward: -19.0\n",
      "self.total_reward: -20.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train\n",
    "env = wb.make_env(DEFAULT_ENV_NAME)\n",
    "myModel = dmb.MyModel(env.observation_space.shape, env.action_space.n)\n",
    "myModel.load_weights(\"Weights_store/PongModelWeights999999-new1.h5\")\n",
    "agent = Agent(env)\n",
    "his_myModel = myModel.export_model()\n",
    "for i in range(EPISODES):\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - i / EPSILON_DECAY_LAST_FRAME)\n",
    "    agent.play_step(myModel, epsilon)\n",
    "    exp_buffer = agent.get_exp_buffer()\n",
    "    if i > START_STEPS:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "memories = sample_memories2(exp_buffer, 100)\n",
    "current_obs_v = memories[0]\n",
    "action_v = memories[1]\n",
    "reward_v = memories[2]\n",
    "done_v = memories[3]\n",
    "next_obs_v = memories[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[4 4 1 5 1 1 5 1 5 1 3 5 3 3 3 4 1 5 1 5 2 4 4 3 1 2 5 1 1 5 4 3 4 5 4 3 4\n",
      " 3 4 5 4 3 1 4 4 4 3 4 3 1 5 5 3 4 5 3 5 4 3 4 5 5 5 3 3 5 4 5 5 5 4 3 4 4\n",
      " 4 1 4 1 1 1 1 3 3 3 5 0 4 5 4 5 4 4 5 1 4 1 4 3 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(done_v)\n",
    "print(reward_v)\n",
    "print(action_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [6.274074  6.468874  6.4192467 6.4074264 6.489381  6.592574 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [6.708174  7.0253286 6.8722754 7.005249  7.2433567 7.231888 ]\n",
      " [6.274074  6.468874  6.4192467 6.4074264 6.489381  6.592574 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]]\n"
     ]
    }
   ],
   "source": [
    "Qas = myModel.predict(current_obs_v[:12], 32)\n",
    "print(Qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [6.274074  6.468874  6.4192467 6.4074264 6.489381  6.592574 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [6.708174  7.0253286 6.8722754 7.005249  7.2433567 7.231888 ]\n",
      " [6.274074  6.468874  6.4192467 6.4074264 6.489381  6.592574 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [5.897474  5.9980726 6.0468388 6.0660343 5.9586926 6.066531 ]\n",
      " [6.4777226 6.5217366 6.4755254 6.5926943 6.439637  6.680376 ]]\n"
     ]
    }
   ],
   "source": [
    "Qas_next = his_myModel.predict(next_obs_v[:12])\n",
    "print(Qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.680376  6.592574  6.680376  6.066531  6.680376  7.2433567 6.592574\n",
      " 6.066531  6.680376  6.066531  6.066531  6.680376 ]\n"
     ]
    }
   ],
   "source": [
    "max_reward_v = np.max(Qas_next, axis=-1)\n",
    "print(max_reward_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-done_v[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.613572   6.5266485  6.613572   6.005866   6.613572   7.170923\n",
      "  5.5266485  6.005866   6.613572   6.005866   6.005866  -1.       ]\n"
     ]
    }
   ],
   "source": [
    "expect = reward_v[:12] + (GAMMA * max_reward_v) * (1 - done_v[:12])\n",
    "print(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.4777226  6.5217366  6.4755254  6.5926943  6.613572   6.680376 ]\n",
      " [ 6.274074   6.468874   6.4192467  6.4074264  6.5266485  6.592574 ]\n",
      " [ 6.4777226  6.613572   6.4755254  6.5926943  6.439637   6.680376 ]\n",
      " [ 5.897474   5.9980726  6.0468388  6.0660343  5.9586926  6.005866 ]\n",
      " [ 6.4777226  6.613572   6.4755254  6.5926943  6.439637   6.680376 ]\n",
      " [ 6.708174   7.170923   6.8722754  7.005249   7.2433567  7.231888 ]\n",
      " [ 6.274074   6.468874   6.4192467  6.4074264  6.489381   5.5266485]\n",
      " [ 5.897474   6.005866   6.0468388  6.0660343  5.9586926  6.066531 ]\n",
      " [ 6.4777226  6.5217366  6.4755254  6.5926943  6.439637   6.613572 ]\n",
      " [ 5.897474   6.005866   6.0468388  6.0660343  5.9586926  6.066531 ]\n",
      " [ 5.897474   5.9980726  6.0468388  6.005866   5.9586926  6.066531 ]\n",
      " [ 6.4777226  6.5217366  6.4755254  6.5926943  6.439637  -1.       ]]\n"
     ]
    }
   ],
   "source": [
    "indicis = one_hot(action_v[:12], Qas.shape)\n",
    "Qas[indicis.nonzero()] = expect\n",
    "print(Qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
